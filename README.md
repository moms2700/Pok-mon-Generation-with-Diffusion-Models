# ğŸ® PokÃ©mon Diffusion Model - GÃ©nÃ©ration d'Images avec Deep Learning

## ğŸ“‹ Description du Projet

Ce projet implÃ©mente un systÃ¨me de gÃ©nÃ©ration d'images de PokÃ©mon utilisant des modÃ¨les de diffusion (DDPM - Denoising Diffusion Probabilistic Models). Le systÃ¨me utilise une architecture U-Net conditionnÃ©e par des embeddings textuels CLIP pour gÃ©nÃ©rer des images de PokÃ©mon Ã  partir de descriptions textuelles incluant leur nom et leurs types.

## ğŸš€ CaractÃ©ristiques Principales

- **Web Scraping AutomatisÃ©** : Collecte automatique des images et mÃ©tadonnÃ©es de PokÃ©mon depuis Bulbapedia
- **ModÃ¨le de Diffusion Conditionnel** : GÃ©nÃ©ration d'images basÃ©e sur des embeddings textuels CLIP
- **Architecture U-Net OptimisÃ©e** : ImplÃ©mentation personnalisÃ©e avec connexions rÃ©siduelles et attention
- **Visualisation en Temps RÃ©el** : Suivi du processus de gÃ©nÃ©ration Ã©tape par Ã©tape
- **Support GPU** : Optimisation complÃ¨te pour l'entraÃ®nement sur GPU

## ğŸ“Š Ã‰volution du Projet

Le projet prÃ©sente trois implÃ©mentations successives avec des amÃ©liorations progressives :

### Version 1 - ImplÃ©mentation Initiale
- Architecture basique avec problÃ¨mes de mÃ©moire
- Images 128x128, rÃ©sultats trÃ¨s bruitÃ©s
- Temps d'entraÃ®nement : >24h

### Version 2 - Optimisations
- RÃ©duction Ã  32x32 pixels pour efficacitÃ©
- Meilleure gestion mÃ©moire
- Temps d'entraÃ®nement : 12h

### Version 3 - ImplÃ©mentation Finale âœ…
- Architecture U-Net amÃ©liorÃ©e
- Visualisation du processus de diffusion
- Temps d'entraÃ®nement : 6h
- RÃ©sultats de qualitÃ© significativement supÃ©rieure

## ğŸ› ï¸ Technologies UtilisÃ©es

- **TensorFlow/Keras** : Framework de deep learning
- **CLIP (OpenAI)** : GÃ©nÃ©ration d'embeddings textuels
- **BeautifulSoup** : Web scraping
- **NumPy/OpenCV** : Traitement d'images
- **Matplotlib** : Visualisation

## ğŸ“ Structure du Projet

```
pokemon-diffusion/
â”œâ”€â”€ data_collection/
â”‚   â”œâ”€â”€ scraper.py          # Web scraping depuis Bulbapedia
â”‚   â””â”€â”€ preprocessor.py     # Traitement et embeddings CLIP
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ unet.py            # Architecture U-Net
â”‚   â”œâ”€â”€ diffusion.py       # Processus DDPM
â”‚   â””â”€â”€ training.py        # Boucle d'entraÃ®nement
â”œâ”€â”€ generation/
â”‚   â””â”€â”€ sampler.py         # GÃ©nÃ©ration d'images
â”œâ”€â”€ pokemon_images/        # Images collectÃ©es
â”œâ”€â”€ embeddings/           # Embeddings CLIP
â””â”€â”€ results/             # Images gÃ©nÃ©rÃ©es
```

## ğŸš¦ Installation et Utilisation

```bash
# Cloner le repository
git clone https://github.com/[username]/pokemon-diffusion.git
cd pokemon-diffusion

# Installer les dÃ©pendances
pip install -r requirements.txt

# Collecter les donnÃ©es
python data_collection/scraper.py

# EntraÃ®ner le modÃ¨le
python models/training.py

# GÃ©nÃ©rer des PokÃ©mon
python generation/sampler.py --name "Charizard" --types "Fire,Flying"
```

## ğŸ“ˆ Performances

| MÃ©trique | Version Finale |
|----------|---------------|
| RÃ©solution | 32x32 pixels |
| Temps d'entraÃ®nement | ~6 heures |
| MÃ©moire GPU | 2-4GB |
| Vitesse de gÃ©nÃ©ration | <5 secondes |
| Dataset | ~900 PokÃ©mon |

## ğŸ¯ RÃ©sultats

Le modÃ¨le est capable de gÃ©nÃ©rer des images reconnaissables de PokÃ©mon avec :
- Formes cohÃ©rentes selon les types
- Couleurs appropriÃ©es
- Structures caractÃ©ristiques des PokÃ©mon

## âš ï¸ Limitations

- RÃ©solution limitÃ©e Ã  32x32 pixels
- DÃ©tails fins parfois manquants
- Dataset relativement petit (~900 images)
- L'approche avec encodeur latent (4Ã¨me tentative) s'est rÃ©vÃ©lÃ©e moins efficace

## ğŸ”® AmÃ©liorations Futures

- [ ] Augmenter la rÃ©solution des images gÃ©nÃ©rÃ©es
- [ ] ImplÃ©menter des mÃ©canismes d'attention plus sophistiquÃ©s
- [ ] Explorer l'augmentation de donnÃ©es pour enrichir le dataset
- [ ] Optimiser l'architecture pour des images 64x64 ou 128x128
- [ ] Ajouter un systÃ¨me de contrÃ´le plus fin (pose, expression, etc.)

## ğŸ“š Apprentissages ClÃ©s

Ce projet dÃ©montre que :
1. La simplicitÃ© architecturale peut surpasser la complexitÃ© dans les modÃ¨les de diffusion
2. La prÃ©servation de la dimensionnalitÃ© est cruciale pour la qualitÃ©
3. L'approche directe sur les pixels est plus efficace que l'utilisation d'espaces latents pour des datasets limitÃ©s

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :
- Signaler des bugs
- Proposer des amÃ©liorations
- Soumettre des pull requests

## ğŸ“„ Licence

Ce projet est Ã  des fins Ã©ducatives et de recherche uniquement.
